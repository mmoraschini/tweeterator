{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import tweeterator_pos\n",
    "from loader import Loader\n",
    "from data_generator_pos import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'data/trump.csv'\n",
    "text_column = 'text'\n",
    "file_type = 'csv'\n",
    "\n",
    "w_net_type = 'LSTM'\n",
    "w_latent_dim = 60\n",
    "w_n_units = 256\n",
    "w_dropout = 0.2\n",
    "w_n_hidden_layers = 1\n",
    "\n",
    "pos_net_type = 'LSTM'\n",
    "pos_latent_dim = 8\n",
    "pos_n_units = 64\n",
    "pos_dropout = 0\n",
    "pos_n_hidden_layers = 1\n",
    "\n",
    "window = 5\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "perc_val = 0.2\n",
    "regex_to_remove = ['^rt ']\n",
    "shuffle = True\n",
    "train_two_nets = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(flatten_hashtags=False, flatten_mentions=False)\n",
    "data = loader.load(input, file_type=file_type, text_column=text_column, window=window, regex_to_remove=regex_to_remove)\n",
    "data = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at loaded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words\n",
    "print(f\"Number of words: {len(list(itertools.chain(*data)))}\")\n",
    "print(f\"Number of unique words: {len(set(itertools.chain(*data)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words that appear only once (probably typos, errors, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_text = list(itertools.chain(*data))\n",
    "vc = pd.value_counts(flattened_text)\n",
    "words_to_remove = vc[vc == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in data:\n",
    "    count = 0\n",
    "    for word in list(sentence):\n",
    "        if word in words_to_remove:\n",
    "            sentence.remove(word)\n",
    "            count += 1\n",
    "    \n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of affected sentences: {np.sum(np.array(counts) > 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of sentences: {len(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_sentences = []\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i]\n",
    "    if len(sentence) == 0:\n",
    "        empty_sentences.append(i)\n",
    "\n",
    "data = np.delete(data, empty_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data[:5]:\n",
    "    print('-' + ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plus_pos = []\n",
    "for sentence in tqdm(data):\n",
    "    sentence_plus_pos = nltk.pos_tag(sentence)\n",
    "    sentence_plus_pos = [list(pos) for pos in sentence_plus_pos]\n",
    "    data_plus_pos.append(sentence_plus_pos)\n",
    "data_plus_pos = np.array(data_plus_pos, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and get the trained model, the history and the word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, dicts, _ = tweeterator_pos.train(data_plus_pos, window, batch_size, epochs, perc_val, shuffle, learning_rate, train_two_nets,\n",
    "                                                 w_net_type, w_latent_dim, w_n_units, w_dropout, w_n_hidden_layers,\n",
    "                                                 pos_net_type, pos_latent_dim, pos_n_units, pos_dropout, pos_n_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sentence and start generating text from its first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = dicts['word2int']\n",
    "i2w = dicts['int2word']\n",
    "pos2i = dicts['pos2int']\n",
    "i2pos = dicts['int2pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data_plus_pos[np.random.choice(data_plus_pos.size, 1)]\n",
    "gen = DataGenerator(sentence, w2i, pos2i, window, 1, shuffle=False)\n",
    "test_example = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 40\n",
    "output_int_word = np.empty(output_size, dtype=int)\n",
    "\n",
    "# Set the first window elements to the start of the phrase\n",
    "output_int_word[:window] = [word for word in test_example[0][0][0]]\n",
    "\n",
    "# Predict the next word from the preceding ones (using the words already predicted)\n",
    "for i in range(0, output_int_word.size - window):\n",
    "    input_int_word = output_int_word[np.newaxis, i:window+i, np.newaxis]\n",
    "\n",
    "    input_str = [i2w[ii] for ii in output_int_word[i:window+i]]\n",
    "    words_and_pos_tags = nltk.pos_tag(input_str)\n",
    "    pos_tags = list(zip(*words_and_pos_tags))[1]\n",
    "    pos_tags_int = np.array([pos2i[pos] for pos in pos_tags])\n",
    "    pos_tags_int = pos_tags_int[np.newaxis, :, np.newaxis]\n",
    "    \n",
    "    # Even if the whole sentence is known in this test, run POS tagging only on the part\n",
    "    # of sentence preceding the word to generate (real-like scenario)\n",
    "    input_str = [i2w[ii] for ii in output_int_word[i:window+i]]\n",
    "    \n",
    "    if train_two_nets:\n",
    "        prediction = model[0]([input_int_word, pos_tags_int]).numpy()[0]\n",
    "        prediction_pos = model[1]([input_int_word, pos_tags_int]).numpy()[0]\n",
    "\n",
    "        best_guesses = np.argsort(prediction)[::-1][:10]\n",
    "        posterior = []\n",
    "        for guess in best_guesses:\n",
    "            # Get the POS tag for the examined word\n",
    "            test_str = input_str + [i2w[guess]]\n",
    "            guess_pos = nltk.pos_tag(test_str)[-1][1]\n",
    "\n",
    "            prior = prediction[guess]\n",
    "\n",
    "            # Get index of POS in prediction output\n",
    "            pos_i = pos2i[guess_pos]\n",
    "            pos_prob = prediction_pos[pos_i]\n",
    "            posterior.append(prior * pos_prob)\n",
    "\n",
    "        if deterministic:\n",
    "            chosen_guess = np.argmax(posterior)\n",
    "        else:\n",
    "            posterior = np.array(posterior) / np.sum(posterior)\n",
    "            chosen_guess = np.random.choice(range(len(posterior)), 1, p=posterior)[0]\n",
    "        \n",
    "        word_int = best_guesses[chosen_guess]\n",
    "    else:\n",
    "        prediction = model([input_int_word, pos_tags_int]).numpy()[0]\n",
    "        if deterministic:\n",
    "            word_int = np.argmax(prediction)\n",
    "        else:\n",
    "            word_int = np.random.choice(range(len(prediction)), 1, p=prediction)[0]\n",
    "\n",
    "    output_int_word[window + i] = word_int\n",
    "\n",
    "# Convert integers to words\n",
    "output = []\n",
    "for i in range(len(output_int_word)):\n",
    "    word_int = output_int_word[i]\n",
    "    word = i2w[word_int]\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the produced output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28d49bc86e1b30cc53634441a90f77c016f9cae5b9cd2f4304fa8d50ea4bfc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('text': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
