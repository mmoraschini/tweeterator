{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tweeterator\n",
    "from loader import Loader\n",
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'data/vicinitas_user_tweets.xlsx'\n",
    "net_type = 'RNN'\n",
    "latent_dim = 50\n",
    "window = 5\n",
    "dropout = 0\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "perc_test = 0.2\n",
    "n_hidden_layers = 2\n",
    "regex_to_remove = ['^#.+:\\s+']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and get the trained model and the word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, w2i, i2w = tweeterator.train(input, net_type, latent_dim, window, dropout, batch_size, epochs, learning_rate,\n",
    "                          perc_test, n_hidden_layers, regex_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test starting from the first words of the first training phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader()\n",
    "data = loader.load(input, window=window + 1, regex_to_remove=regex_to_remove)\n",
    "generator = DataGenerator(data, w2i, window, batch_size)\n",
    "test_example = next(generator)[0]\n",
    "\n",
    "output_size = 40\n",
    "output_int = np.empty(output_size, dtype=int)\n",
    "\n",
    "# Set the first window elements to the start of the phrase\n",
    "output_int[:window] = test_example[0]\n",
    "\n",
    "# Predict the next word from the preceding ones (using the predicted words)\n",
    "for i in range(0, output_int.size - window):\n",
    "    input_int = output_int[np.newaxis, i:window + i, np.newaxis]\n",
    "    prediction = model(input_int)\n",
    "    word_int = np.argmax(prediction)\n",
    "    output_int[window + i] = word_int\n",
    "\n",
    "# Convert integers to words\n",
    "output = []\n",
    "for i in range(len(output_int)):\n",
    "    word_int = output_int[i]\n",
    "    word = i2w[word_int]\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the produced output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28d49bc86e1b30cc53634441a90f77c016f9cae5b9cd2f4304fa8d50ea4bfc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('text': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
