{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tweeterator\n",
    "from loader import Loader\n",
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ''\n",
    "text_column = 'text'\n",
    "file_type = 'csv'\n",
    "net_type = 'LSTM'\n",
    "latent_dim = 64\n",
    "n_units = 256\n",
    "window = 10\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "perc_val = 0.2\n",
    "n_hidden_layers = 1\n",
    "regex_to_remove = ['^rt ']\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(flatten_hashtags=False, flatten_mentions=False)\n",
    "data = loader.load(input, file_type=file_type, text_column=text_column, window=window, regex_to_remove=regex_to_remove)\n",
    "data = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at loaded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words\n",
    "print(f\"Number of words: {len(list(itertools.chain(*data)))}\")\n",
    "print(f\"Number of unique words: {len(set(itertools.chain(*data)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words that appear only once (probably typos, errors, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_text = list(itertools.chain(*data))\n",
    "vc = pd.value_counts(flattened_text)\n",
    "words_to_remove = vc[vc == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in data:\n",
    "    count = 0\n",
    "    for word in list(sentence):\n",
    "        if word in words_to_remove:\n",
    "            sentence.remove(word)\n",
    "            count += 1\n",
    "    \n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of affected sentences: {np.sum(np.array(counts) > 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of sentences: {len(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data[:5]:\n",
    "    print('-' + ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and get the trained model, the history and the word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, dicts, _ = tweeterator.train(data, net_type, latent_dim, n_units, window, dropout, batch_size, epochs, learning_rate,\n",
    "                                             perc_val, n_hidden_layers, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sentence and start generating text from its first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = dicts['word2int']\n",
    "i2w = dicts['int2word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[np.random.choice(data.size, 1)]\n",
    "gen = DataGenerator(sentence, w2i, window, 1, shuffle=False)\n",
    "test_example = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 40\n",
    "output_int = np.empty(output_size, dtype=int)\n",
    "\n",
    "# Set the first window elements to the start of the phrase\n",
    "output_int[:window] = test_example[0]\n",
    "\n",
    "# Predict the next word from the preceding ones (using the words already predicted)\n",
    "for i in range(0, output_int.size - window):\n",
    "    input_int = output_int[np.newaxis, i:window + i, np.newaxis]\n",
    "    prediction = model(input_int).numpy()[0]\n",
    "    # Activate this to generate randomly by sampling based on the probabilities\n",
    "    #word_int = np.random.choice(range(len(prediction)), 1, p=prediction)[0]\n",
    "    # Activate this to make it deterministic\n",
    "    word_int = np.argmax(prediction)\n",
    "    output_int[window + i] = word_int\n",
    "\n",
    "# Convert integers to words\n",
    "output = []\n",
    "for i in range(len(output_int)):\n",
    "    word_int = output_int[i]\n",
    "    word = i2w[word_int]\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the produced output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28d49bc86e1b30cc53634441a90f77c016f9cae5b9cd2f4304fa8d50ea4bfc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('text': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
