{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tweeterator\n",
    "from loader import Loader\n",
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'data/trump.csv'\n",
    "text_column = 'text'\n",
    "file_type = 'csv'\n",
    "net_type = 'LSTM'\n",
    "latent_dim = 64\n",
    "n_units = 256\n",
    "window = 5\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "perc_val = 0.2\n",
    "n_hidden_layers = 1\n",
    "regex_to_remove = ['^rt ']\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(flatten_hashtags=False, flatten_mentions=False)\n",
    "data = loader.load(input, file_type=file_type, text_column=text_column, window=window, regex_to_remove=regex_to_remove)\n",
    "data = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at loaded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words\n",
    "print(f\"Number of words: {len(list(itertools.chain(*data)))}\")\n",
    "print(f\"Number of unique words: {len(set(itertools.chain(*data)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words that appear only once (probably typos, errors, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_text = list(itertools.chain(*data))\n",
    "vc = pd.value_counts(flattened_text)\n",
    "words_to_remove = vc[vc == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in data:\n",
    "    count = 0\n",
    "    for word in list(sentence):\n",
    "        if word in words_to_remove:\n",
    "            sentence.remove(word)\n",
    "            count += 1\n",
    "    \n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of affected sentences: {np.sum(np.array(counts) > 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of sentences: {len(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_sentences = []\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i]\n",
    "    if len(sentence) == 0:\n",
    "        empty_sentences.append(i)\n",
    "\n",
    "data = np.delete(data, empty_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data[:5]:\n",
    "    print('-' + ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequencies of part-of-speech (POS) tags. These will be taken into consideration when choosing the next word to generate\n",
    "dict_pos_freq = {}\n",
    "dict_pos_count = {}\n",
    "\n",
    "for sentence in tqdm(data):\n",
    "    words_and_pos_tags = nltk.pos_tag(sentence)\n",
    "    pos_tags = list(zip(*words_and_pos_tags))[1]\n",
    "    for i in range(len(sentence) - window):\n",
    "        preceding = str(pos_tags[i:i+window])\n",
    "        following = pos_tags[i+window]\n",
    "        try:\n",
    "            dict_pos_count[preceding] += 1\n",
    "            try:\n",
    "                # Increase the frequency of seeing the window preceding POSs and then the following POS\n",
    "                dict_pos_freq[preceding][following] += 1\n",
    "            except KeyError:\n",
    "                dict_pos_freq[preceding][following] = 1\n",
    "        except KeyError:\n",
    "            dict_pos_freq[preceding] = {following: 1}\n",
    "            dict_pos_count[preceding] = 1\n",
    "\n",
    "# Normalise each count to obtain a frequency\n",
    "for k1 in dict_pos_freq.keys():\n",
    "    v1 = dict_pos_freq[k1]\n",
    "    count = dict_pos_count[k1]\n",
    "    for k2 in v1.keys():\n",
    "        v1[k2] = v1[k2] / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum observed frequency, to assign as default when the combination of preceding POS tags and predicted POS\n",
    "# has not been observed in the texts\n",
    "min_freq = 1\n",
    "for k1 in dict_pos_freq.keys():\n",
    "    v1 = dict_pos_freq[k1]\n",
    "    for k2 in v1.keys():\n",
    "        if v1[k2] < min_freq:\n",
    "            min_freq = v1[k2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and get the trained model, the history and the word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, dicts, _ = tweeterator.train(data, net_type, latent_dim, n_units, window, dropout, batch_size, epochs, learning_rate,\n",
    "                                             perc_val, n_hidden_layers, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sentence and start generating text from its first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = dicts['word2int']\n",
    "i2w = dicts['int2word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[np.random.choice(data.size, 1)]\n",
    "gen = DataGenerator(sentence, w2i, window, 1, shuffle=False)\n",
    "test_example = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic = False\n",
    "use_pos_info = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 40\n",
    "output_int = np.empty(output_size, dtype=int)\n",
    "\n",
    "# Set the first window elements to the start of the phrase\n",
    "output_int[:window] = test_example[0]\n",
    "\n",
    "# Predict the next word from the preceding ones (using the words already predicted)\n",
    "for i in range(0, output_int.size - window):\n",
    "    input_int = output_int[np.newaxis, i:window+i, np.newaxis]\n",
    "    prediction = model(input_int).numpy()[0]\n",
    "    \n",
    "    if use_pos_info:\n",
    "        # Even if the whole sentence is known in this test, run POS tagging only on the part\n",
    "        # of sentence preceding the word to generate (real-like scenario)\n",
    "        input_str = [i2w[ii] for ii in output_int[i:window+i]]\n",
    "        words_and_pos_tags = nltk.pos_tag(input_str)\n",
    "        pos_tags = list(zip(*words_and_pos_tags))[1]\n",
    "        preceding = str(pos_tags)\n",
    "        try:\n",
    "            pos_freqs = dict_pos_freq[preceding]\n",
    "        except KeyError:\n",
    "            pos_freqs = None\n",
    "        \n",
    "        best_guesses = np.argsort(prediction)[::-1][:10]\n",
    "        posterior = []\n",
    "        for guess in best_guesses:\n",
    "            # Get the POS tag for the examined word\n",
    "            test_str = input_str + [i2w[guess]]\n",
    "\n",
    "            guess_pos = nltk.pos_tag(test_str)[-1][1]\n",
    "            try:\n",
    "                pos_prob = pos_freqs[guess_pos]\n",
    "            except KeyError:\n",
    "                # If this sequence was never observed give it a probability equal to the minimum frequence observed\n",
    "                pos_prob = min_freq\n",
    "            except TypeError:\n",
    "                # if pos_freqs is None because the sequence was never observed, do not vary the probabilities\n",
    "                pos_prob = 1\n",
    "\n",
    "            prior = prediction[guess]\n",
    "\n",
    "            posterior.append(prior * pos_prob)\n",
    "    \n",
    "        if deterministic:\n",
    "            chosen_guess = np.argmax(posterior)\n",
    "        else:\n",
    "            posterior = np.array(posterior) / np.sum(posterior)\n",
    "            chosen_guess = np.random.choice(range(len(posterior)), 1, p=posterior)[0]\n",
    "        \n",
    "        word_int = best_guesses[chosen_guess]\n",
    "    else:\n",
    "        if deterministic:\n",
    "            word_int = np.argmax(prediction)\n",
    "        else:\n",
    "            word_int = np.random.choice(range(len(prediction)), 1, p=prediction)[0]\n",
    "\n",
    "    output_int[window + i] = word_int\n",
    "\n",
    "# Convert integers to words\n",
    "output = []\n",
    "for i in range(len(output_int)):\n",
    "    word_int = output_int[i]\n",
    "    word = i2w[word_int]\n",
    "    output.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the produced output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28d49bc86e1b30cc53634441a90f77c016f9cae5b9cd2f4304fa8d50ea4bfc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('text': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
