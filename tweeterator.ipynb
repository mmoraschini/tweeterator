{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import config\n",
    "\n",
    "import pos_tagging\n",
    "import sentence_generation as sg\n",
    "import nets\n",
    "from loader import Loader\n",
    "from generators import SingleDataGenerator, DoubleDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "try:\n",
    "    if gpus:\n",
    "        print('GPU found')\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'data/trump.csv'\n",
    "text_column = 'text'\n",
    "file_type = 'csv'\n",
    "\n",
    "w_net_type = 'LSTM'\n",
    "w_latent_dim = 60\n",
    "w_n_units = 256\n",
    "w_dropout = 0.2\n",
    "w_n_hidden_layers = 1\n",
    "\n",
    "pos_net_type = 'LSTM'\n",
    "pos_latent_dim = 8\n",
    "pos_n_units = 64\n",
    "pos_dropout = 0\n",
    "pos_n_hidden_layers = 1\n",
    "\n",
    "window = 10\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "perc_val = 0.2\n",
    "regex_replace = {'\"': '', '^rt ': '', '&amp.': 'and', 'â€™': '\\'',\n",
    "                 'http.*\\w': 'http://website.com', '\\.\\.\\.': ''}\n",
    "allowed_symbols = ['\\'', '\"', '/', ':', '#', '@']\n",
    "shuffle = True\n",
    "train_two_nets = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(flatten_hashtags=False, flatten_mentions=True)\n",
    "data = loader.load(input, file_type=file_type, text_column=text_column, window=window,\n",
    "                   regex_replace=regex_replace, allowed_symbols=allowed_symbols)\n",
    "data = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at loaded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words\n",
    "print(f\"Number of words: {len(list(itertools.chain(*data)))}\")\n",
    "print(f\"Number of unique words: {len(set(itertools.chain(*data)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words that appear only once (probably typos, errors, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_text = list(itertools.chain(*data))\n",
    "vc = pd.value_counts(flattened_text)\n",
    "words_to_remove = vc[vc == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in data:\n",
    "    count = 0\n",
    "    for word in list(sentence):\n",
    "        if word in words_to_remove:\n",
    "            sentence.remove(word)\n",
    "            count += 1\n",
    "    \n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of sentences: {len(counts)}')\n",
    "print(f'Number of affected sentences: {np.sum(np.array(counts) > 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_sentences = []\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i]\n",
    "    if len(sentence) == 0:\n",
    "        empty_sentences.append(i)\n",
    "\n",
    "print(f'Number of deleted sentences: {len(empty_sentences)}')\n",
    "data = np.delete(data, empty_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data[:5]:\n",
    "    print('-' + ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plus_pos = pos_tagging.get_tags(data)\n",
    "dict_pos_freq, dict_pos_count, min_freq = pos_tagging.get_frequency(data, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate train-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_phrases = len(data)\n",
    "val_idx = np.random.choice(np.arange(n_phrases), int(n_phrases * perc_val), replace=False)\n",
    "train_idx = np.setdiff1d(np.arange(n_phrases), val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train_data = data[train_idx]\n",
    "word_val_data = data[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_data = [[tok[1] for tok in sentence] for sentence in data_plus_pos[train_idx]]\n",
    "pos_val_data = [[tok[1] for tok in sentence] for sentence in data_plus_pos[val_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_train_data = data_plus_pos[train_idx]\n",
    "double_val_data = data_plus_pos[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get conversion dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "pos2int = {}\n",
    "int2pos = {}\n",
    "for sent in data_plus_pos:\n",
    "    for tok in sent:\n",
    "        if tok[0] not in word2int.keys():\n",
    "            idx = len(word2int)\n",
    "            word2int[tok[0]] = idx\n",
    "            int2word[idx] = tok[0]\n",
    "        \n",
    "        if tok[1] not in pos2int.keys():\n",
    "            idx = len(pos2int)\n",
    "            pos2int[tok[1]] = idx\n",
    "            int2pos[idx] = tok[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train_data_generator = SingleDataGenerator(word_train_data, word2int, window, batch_size, shuffle)\n",
    "word_val_data_generator = SingleDataGenerator(word_val_data, word2int, window, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_data_generator = SingleDataGenerator(pos_train_data, pos2int, window, batch_size, shuffle)\n",
    "pos_val_data_generator = SingleDataGenerator(pos_val_data, pos2int, window, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_train_data_generator = DoubleDataGenerator(double_train_data, word2int, pos2int, window, batch_size, shuffle)\n",
    "double_val_data_generator = DoubleDataGenerator(double_val_data, word2int, pos2int, window, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first elements of a sentence to test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 52\n",
    "np.random.seed = seed\n",
    "rnd_int = np.random.randint(0, len(double_val_data))\n",
    "rnd_sentence = double_val_data[rnd_int]\n",
    "\n",
    "start_of_sentence_word = [ii[0] for ii in rnd_sentence[:window]]\n",
    "start_of_sentence_pos = [ii[1] for ii in rnd_sentence[:window]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following sections will train and test one of the possible network models and generation modalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = nets.one_input_one_output(window, w_net_type, len(word2int), w_latent_dim, w_n_units, w_dropout, w_n_hidden_layers)\n",
    "optim_adam = Adam(learning_rate=learning_rate)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=optim_adam, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model1.fit(word_train_data_generator, steps_per_epoch=word_train_data_generator.get_n_steps_in_epoch(),\n",
    "                    validation_data=word_val_data_generator, validation_steps=word_val_data_generator.get_n_steps_in_epoch(),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history1.history['loss'], label='loss')\n",
    "plt.plot(history1.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history1.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "plt.plot(history1.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without POS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_1 = sg.one_model_one_input_one_output(model1, start_of_sentence_word, window, use_pos_info=False, w2i=word2int,\n",
    "    i2w=int2word, deterministic=False, output_length=40, pos_freq=None, min_freq=None, stop_at_eos=True)\n",
    "\n",
    "' '.join(res1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With POS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_2 = sg.one_model_one_input_one_output(model1, start_of_sentence_word, window, use_pos_info=True, w2i=word2int,\n",
    "    i2w=int2word, pos_freq=dict_pos_freq, min_freq=min_freq, deterministic=False, output_length=40, stop_at_eos=True)\n",
    "\n",
    "' '.join(res1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model two inputs one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nets.two_inputs_one_output(window, w_net_type, len(word2int), w_latent_dim, w_n_units, w_dropout, w_n_hidden_layers,\n",
    "                                    pos_net_type, len(pos2int), pos_latent_dim, pos_n_units, pos_dropout, pos_n_hidden_layers)\n",
    "optim_adam = Adam(learning_rate=learning_rate)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=optim_adam, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(double_train_data_generator, steps_per_epoch=double_train_data_generator.get_n_steps_in_epoch(),\n",
    "                    validation_data=double_val_data_generator, validation_steps=double_val_data_generator.get_n_steps_in_epoch(),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = sg.one_model_two_inputs_one_output(model2, start_of_sentence_word, window, word2int, int2word, pos2int,\n",
    "    deterministic=False, output_length=40, stop_at_eos=True)\n",
    "' '.join(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history2.history['loss'], label='loss')\n",
    "plt.plot(history2.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history2.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "plt.plot(history2.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model two inputs two outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = nets.two_inputs_two_outputs(window, w_net_type, len(word2int), w_latent_dim, w_n_units, w_dropout, w_n_hidden_layers,\n",
    "                                     pos_net_type, len(pos2int), pos_latent_dim, pos_n_units, pos_dropout, pos_n_hidden_layers)\n",
    "optim_adam = Adam(learning_rate=learning_rate)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=optim_adam, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(double_train_data_generator, steps_per_epoch=double_train_data_generator.get_n_steps_in_epoch(),\n",
    "                      validation_data=double_val_data_generator, validation_steps=double_val_data_generator.get_n_steps_in_epoch(),\n",
    "                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = sg.one_model_two_inputs_two_outputs(model3, start_of_sentence_word, window, word2int, int2word, pos2int,\n",
    "    deterministic=False, output_length=40, stop_at_eos=True)\n",
    "\n",
    "' '.join(res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(history3.history['loss'], 'r', label='loss')\n",
    "plt.plot(history3.history['w_output_loss'], 'ro-', alpha=0.5, label='w_output_loss')\n",
    "plt.plot(history3.history['pos_output_loss'], 'rx-' , alpha=0.5, label='pos_output_loss')\n",
    "plt.plot(history3.history['val_loss'], 'b', label='val_loss')\n",
    "plt.plot(history3.history['val_w_output_loss'], 'bo-', alpha=0.5, label='val_output_loss')\n",
    "plt.plot(history3.history['val_pos_output_loss'], 'bx-', alpha=0.5, label='val_pos_output_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(history3.history['w_output_categorical_accuracy'], 'ro-', alpha=0.5, label='w_output_categorical_accuracy')\n",
    "plt.plot(history3.history['pos_output_categorical_accuracy'], 'rx-' , alpha=0.5, label='pos_output_categorical_accuracy')\n",
    "plt.plot(history3.history['val_w_output_categorical_accuracy'], 'bo-', alpha=0.5, label='val_w_output_categorical_accuracy')\n",
    "plt.plot(history3.history['val_pos_output_categorical_accuracy'], 'bx-', alpha=0.5, label='val_pos_output_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_model = nets.one_input_one_output(window, w_net_type, len(word2int), w_latent_dim, w_n_units, w_dropout, w_n_hidden_layers)\n",
    "pos_model = nets.one_input_one_output(window, pos_net_type, len(pos2int), pos_latent_dim, pos_n_units, pos_dropout, pos_n_hidden_layers)\n",
    "\n",
    "w_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['categorical_accuracy'])\n",
    "pos_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history = w_model.fit(word_train_data_generator, steps_per_epoch=word_train_data_generator.get_n_steps_in_epoch(),\n",
    "                    validation_data=word_val_data_generator, validation_steps=word_val_data_generator.get_n_steps_in_epoch(),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_history = pos_model.fit(pos_train_data_generator, steps_per_epoch=pos_train_data_generator.get_n_steps_in_epoch(),\n",
    "                        validation_data=pos_val_data_generator, validation_steps=pos_val_data_generator.get_n_steps_in_epoch(),\n",
    "                        epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = sg.two_models([w_model, pos_model], start_of_sentence_word, window, word2int, int2word, pos2int,\n",
    "    deterministic=False, output_length=40, stop_at_eos=True)\n",
    "\n",
    "' '.join(res4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(w_history.history['loss'], label='loss word model')\n",
    "plt.plot(w_history.history['val_loss'], label='val_loss word model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(pos_history.history['loss'], label='loss pos model')\n",
    "plt.plot(pos_history.history['val_loss'], label='val_loss pos model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(w_history.history['categorical_accuracy'], label='categorical_accuracy word model')\n",
    "plt.plot(w_history.history['val_categorical_accuracy'], label='val_categorical_accuracy word model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(pos_history.history['categorical_accuracy'], label='categorical_accuracy pos model')\n",
    "plt.plot(pos_history.history['val_categorical_accuracy'], label='val_categorical_accuracy pos model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28d49bc86e1b30cc53634441a90f77c016f9cae5b9cd2f4304fa8d50ea4bfc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('text': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
